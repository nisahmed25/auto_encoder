{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-Copy1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrw6pQluImrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My_tlYoeJXvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d55e6bfd-c966-4622-9ded-ae251157495a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN5_l3SKQ4cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -f /gdrive/My\\ Drive/fashion-mnist_train.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g61tm3YCRCLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -f /gdrive/My\\ Drive/alpha.py .\n",
        "!cp -f /gdrive/My\\ Drive/cnn_utils.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrbdttvxRCOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXYmohQeQ4j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh3U2zGmImrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from alpha import iterate_minibatches\n",
        "from alpha import saveGraph\n",
        "from alpha import lrelu\n",
        "from alpha import make_square\n",
        "from cnn_utils import  conv_layer, fc_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xwe6aMoImr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os, cv2, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, math\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWyEJlBQImr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfe3a234-b701-4ee3-9c6d-6de3aafeb8ef"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swAmAWGImr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d5caac5d-48e3-48e9-e36d-7d7190587e0f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alpha.py      fashion-mnist_train.csv  sample_data  summary_log\n",
            "cnn_utils.py  __pycache__\t       saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEo1kmNdImsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72z-8dvwImsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afc4kyimImsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e46bfa94-72a6-4bb8-9d1a-ab218b56312f"
      },
      "source": [
        "import urllib\n",
        "urllib.request.urlretrieve(\"https://www.kaggle.com/zalando-research/fashionmnist/downloads/fashionmnist.zip\", \"file.zip\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('file.zip', <http.client.HTTPMessage at 0x7fed00276898>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1en_CPl5Oboz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dcb7943-88f9-4d63-bfba-030dbb08c4b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93uP8jnhObru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp -f ./fashion-mnist_train.csv /gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGEXoKqObt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4zyP-qkImsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Designing auto encoder network.\n",
        "'''\n",
        "graph = tf.Graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an088iz7ImsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "    '''\n",
        "    two placeholder to place input and output images. \n",
        "    Technically this will be same image.\n",
        "    '''\n",
        "    X = tf.placeholder(tf.float32,[None,28,28,1])\n",
        "    Y = tf.placeholder(tf.float32,[None,28,28,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC-_MDU3ImsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Encoder network:\n",
        "    This will map the input image to a euclidian space\n",
        "    Name scope has added to view the network clearly on tensorboard\n",
        "'''\n",
        "with graph.as_default():\n",
        "    with tf.name_scope('en-convolutions'):\n",
        "        conv0 = tf.layers.conv2d(X,filters=4,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv0')\n",
        "    with tf.name_scope('en-pooling'):\n",
        "        maxpool0 = tf.layers.max_pooling2d(conv0,pool_size=(2,2),strides=(2,2),name='pool0')     \n",
        "    \n",
        "    with tf.name_scope('en-convolutions'):\n",
        "        conv1 = tf.layers.conv2d(maxpool0,filters=4,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv1')\n",
        "    with tf.name_scope('en-pooling'):\n",
        "        maxpool1 = tf.layers.max_pooling2d(conv1,pool_size=(2,2),strides=(2,2),name='pool1')\n",
        "    \n",
        "    with tf.name_scope('en-convolutions'):\n",
        "        conv2 = tf.layers.conv2d(maxpool1,filters=8,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv2')\n",
        "    with tf.name_scope('en-pooling'):\n",
        "        maxpool2 = tf.layers.max_pooling2d(conv2,pool_size=(2,2),strides=(2,2),name='pool2')\n",
        "    \n",
        "    with tf.name_scope('en-convolutions'):\n",
        "        conv3 = tf.layers.conv2d(maxpool2,filters=16,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv3')\n",
        "    with tf.name_scope('encoding'):\n",
        "        encoded = tf.layers.average_pooling2d(conv3,pool_size=(2,2),strides=(2,2),name='encoding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v85ONPMNImsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fbc8e00-d6b8-421f-a54b-04f6a0b636fc"
      },
      "source": [
        "'''\n",
        "The last tensor, 'encoded',  will be a of shape (?, 4, 4, 16), which then will be flattened to shape (, 256).\n",
        "This will be the compressed representation of the input image\n",
        "'''\n",
        "encoded"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'encoding/encoding/AvgPool:0' shape=(?, 1, 1, 16) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ynKSJzImsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Decoder network:\n",
        "This will map the encodings back to image of same input shape\n",
        "'''\n",
        "with graph.as_default():\n",
        "    with tf.name_scope('decoder'):\n",
        "        upsample1 = tf.layers.conv2d_transpose(encoded,filters=16,kernel_size=3,padding='valid',strides=2,name='upsample1')\n",
        "        conv4 = tf.layers.conv2d(upsample1,filters=16,kernel_size=(3,3),strides=(1,1),padding='SAME',name='conv4',use_bias=True,activation=lrelu)#upsample1 #\n",
        "        \n",
        "        upsample2 = tf.layers.conv2d_transpose(conv4,filters=8,kernel_size=3,padding='valid',strides=2,name='upsample2') \n",
        "        conv5 = tf.layers.conv2d(upsample2,filters=8,kernel_size=(3,3),strides=(1,1),name='conv5',padding='SAME',use_bias=True,activation=lrelu)#upsample2 #\n",
        "        \n",
        "        upsample3 = tf.layers.conv2d_transpose(conv5,filters=8,kernel_size=5,padding='same',strides=2,name='upsample3')\n",
        "        conv6 = tf.layers.conv2d(upsample3,filters=4,kernel_size=(5,5),strides=(1,1),name='conv6',padding='SAME',use_bias=True,activation=lrelu)\n",
        "        \n",
        "        upsample4 = tf.layers.conv2d_transpose(conv6,filters=8,kernel_size=5,padding='same',strides=2,name='upsample4')\n",
        "        conv7 = tf.layers.conv2d(upsample4,filters=4,kernel_size=(5,5),strides=(1,1),name='conv7',padding='SAME',use_bias=True,activation=lrelu)\n",
        "        \n",
        "        logits = tf.layers.conv2d(conv7,filters=1,kernel_size=(3,3),strides=(1,1),name='logits',padding='SAME',use_bias=True)\n",
        "        decoded = tf.sigmoid(logits,name='recon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX4x55zhImsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d62f312-b493-4554-f2b4-daa54cc920ea"
      },
      "source": [
        "logits"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'decoder/logits/BiasAdd:0' shape=(?, 28, 28, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K6Xw-nepImsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for op in graph.get_operations():\n",
        "#      print(op.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv0rO9StImsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir = './train/fashionmnist/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av9wdQcYImsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "657655ee-b5c4-4bf7-b23f-e5595e55fbc2"
      },
      "source": [
        "train = pd.read_csv('./fashion-mnist_train.csv')\n",
        "train = train.drop(columns='label')\n",
        "train.head()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>pixel40</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>92</td>\n",
              "      <td>101</td>\n",
              "      <td>107</td>\n",
              "      <td>100</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>227</td>\n",
              "      <td>...</td>\n",
              "      <td>211</td>\n",
              "      <td>220</td>\n",
              "      <td>214</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>222</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>134</td>\n",
              "      <td>162</td>\n",
              "      <td>191</td>\n",
              "      <td>214</td>\n",
              "      <td>163</td>\n",
              "      <td>146</td>\n",
              "      <td>165</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "      <td>183</td>\n",
              "      <td>112</td>\n",
              "      <td>55</td>\n",
              "      <td>23</td>\n",
              "      <td>72</td>\n",
              "      <td>102</td>\n",
              "      <td>165</td>\n",
              "      <td>160</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>188</td>\n",
              "      <td>163</td>\n",
              "      <td>93</td>\n",
              "      <td>136</td>\n",
              "      <td>...</td>\n",
              "      <td>171</td>\n",
              "      <td>249</td>\n",
              "      <td>207</td>\n",
              "      <td>197</td>\n",
              "      <td>202</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>69</td>\n",
              "      <td>52</td>\n",
              "      <td>45</td>\n",
              "      <td>74</td>\n",
              "      <td>39</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>187</td>\n",
              "      <td>189</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>230</td>\n",
              "      <td>237</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>116</td>\n",
              "      <td>112</td>\n",
              "      <td>136</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>121</td>\n",
              "      <td>102</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   pixel1  pixel2  pixel3  pixel4  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0       0       0       0       0  ...         0         0         0         0\n",
              "1       0       0       0       0  ...         0         0         0         0\n",
              "2       0       0       0       0  ...         0         0         0         0\n",
              "3       0       0       0       1  ...         0         0         0         0\n",
              "4       0       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwnYehN7WIA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80fd3f16-443b-4de1-dbde-e679ec3dcf62"
      },
      "source": [
        "train.label.unique()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 9, 6, 0, 3, 4, 5, 8, 7, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is_DbFWFImsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c52c4659-e8ec-4168-ff74-40d09ef3377c"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWbviKaVImsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ar = np.array(train)\n",
        "train_ar = train_ar.reshape(-1, 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBh2aU5QSVfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9beac3fa-f01e-4a70-aafd-d71e2f575c1a"
      },
      "source": [
        "plt.imshow(train_ar[0].reshape(28,28))\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee_m6egWImsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4988ce2f-0173-484c-f084-de60db2bac1c"
      },
      "source": [
        "train_ar[0].shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzyUN5fImse",
        "colab_type": "code",
        "colab": {},
        "outputId": "9cef55ff-498a-4847-826a-0de49fad41bf"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbVdf52qImsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "    '''\n",
        "    Defining loss function and optimizer\n",
        "    '''\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)\n",
        "    lr = tf.placeholder(tf.float32, shape=[])\n",
        "    cost = tf.reduce_mean(loss, name ='cost')\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=lr, name='opt').minimize(cost) #optimizer\n",
        "    \n",
        "    summaryMerged = tf.summary.merge_all() #For tensorboard\n",
        "    filename=\"./summary_log/run-\"+time.strftime(\"%d%m-%H%M%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpqwrvegImsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "39dd39f0-18e8-437c-964c-20b5c620f34f"
      },
      "source": [
        "'''\n",
        "Ran 6 times 10 epochs with learning rate decay and different mini batch sizes.\n",
        "Every 10 epochs,\n",
        "training had stopped and checked the model performance by ploting 'predicted duplicates' \n",
        "of random products and validated\n",
        "'''\n",
        "\n",
        "minibatch_size = 256\n",
        "epoch = 10\n",
        "learning_rate = 0.00002\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    writer = tf.summary.FileWriter(filename, sess.graph) \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "#     saver.restore(sess, tf.train.latest_checkpoint('./saved_model/'))\n",
        "    for ep in range(epoch):\n",
        "      avg_cost = 0\n",
        "      batch_costs=[]\n",
        "      for i, minibatch in enumerate(iterate_minibatches(train_ar, minibatch_size)):\n",
        "        batch_cost, _ = sess.run([cost, opt], feed_dict={X: minibatch/255,\n",
        "                                                             Y: minibatch/255,\n",
        "                                                            lr: learning_rate})\n",
        "        batch_costs.append(batch_cost)\n",
        "      print(i)\n",
        "      print(\"Epoch: {}/{}...\".format(ep+1, epoch), \\\n",
        "                  \"Training loss: {:.4f}\".format(batch_cost/i))\n",
        "    saver.save(sess, \"./saved_model/model\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234\n",
            "Epoch: 1/10... Training loss: 0.0029\n",
            "234\n",
            "Epoch: 2/10... Training loss: 0.0027\n",
            "234\n",
            "Epoch: 3/10... Training loss: 0.0025\n",
            "234\n",
            "Epoch: 4/10... Training loss: 0.0025\n",
            "234\n",
            "Epoch: 5/10... Training loss: 0.0022\n",
            "234\n",
            "Epoch: 6/10... Training loss: 0.0021\n",
            "234\n",
            "Epoch: 7/10... Training loss: 0.0020\n",
            "234\n",
            "Epoch: 8/10... Training loss: 0.0018\n",
            "234\n",
            "Epoch: 9/10... Training loss: 0.0018\n",
            "234\n",
            "Epoch: 10/10... Training loss: 0.0016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WwsVKLqImsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FNBI75EImsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9b5c19d4-98ac-4f06-ebbb-0bbf4a60efea"
      },
      "source": [
        "train1 = pd.read_csv('./fashion-mnist_train.csv')\n",
        "train1 = np.array(train1)\n",
        "train1[:,:1]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [9],\n",
              "       [6],\n",
              "       ...,\n",
              "       [8],\n",
              "       [8],\n",
              "       [7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQwYZcddImsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4KQBtgrNJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90b485d7-f7c5-4109-b2f6-e163c813ea00"
      },
      "source": [
        "len(train_ar)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVy0OlZBrZ9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "fd7860e1-e932-4432-ae2c-f4f5536e7924"
      },
      "source": [
        "for i in range(60000):\n",
        "  path = train_ar[i]\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "  img = img/255.0"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-0cae9f04b84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: bad argument type for built-in operation"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLW4PrUPraHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij4Aq0InImsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "eb2008a4-50dd-4567-8526-494c1e62fded"
      },
      "source": [
        "'''\n",
        "Read all images; \n",
        "Calculate embeddings for each images\n",
        "Append the embeddings and product index to two seperate arrays\n",
        "'''\n",
        "limit = 99999\n",
        "IMG_SIZE = 64\n",
        "embeddings = []\n",
        "labels = []\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('./saved_model/'))\n",
        "    for i, im in (enumerate(range(len(train_ar)))):\n",
        "      img = train_ar[im].reshape(1,28,28,1)\n",
        " \n",
        "          \n",
        "      img = img/255.0\n",
        "    \n",
        "      embeddings.append(sess.run(encoded,feed_dict={X:img}).reshape(-1,16))\n",
        "      if i%10000 ==0:\n",
        "        print(i)\n",
        "#       labels.append(int(path.split('.')[-2].split('-')[-1]))  \n",
        "#             print(int(path.split('.')[-2].split('-')[-1]))\n",
        "# labels = np.array(labels)\n",
        "embeddings = np.array(np.squeeze(embeddings))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lliykKCXImsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for op in graph2.get_operations():\n",
        "#      print(op.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lbBPbcoImsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d7b899d4-f934-4345-e29e-971b4d1b08a3"
      },
      "source": [
        "'''\n",
        "Writing the weights to pb file for prediction/serving\n",
        "'''\n",
        "graph2 = tf.Graph()\n",
        "with tf.Session(graph=graph2) as sess:\n",
        "    saver = tf.train.import_meta_graph('./saved_model/model.meta')\n",
        "    input_graph_def = tf.get_default_graph().as_graph_def()\n",
        "    saver.restore(sess, \"./saved_model/model\")\n",
        "    \n",
        "    output_node_names=\"encoding/encoding/AvgPool\"\n",
        "    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n",
        "                                                      input_graph_def,\n",
        "                                                      output_node_names.split(\",\"))\n",
        "    output_graph=\"./pb/TunicsEncoder-v0.1.pb\"\n",
        "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
        "        f.write(output_graph_def.SerializeToString())"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model\n",
            "INFO:tensorflow:Froze 8 variables.\n",
            "INFO:tensorflow:Converted 8 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcdtzIs-VJsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir pb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncTQJLOCVZgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir Embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jP5XHcNVn1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a93a050a-5777-46b3-c2f1-9304565f0a14"
      },
      "source": [
        "960000/256\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3750.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6zWIEE9Imso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Dumping and saving  to .npy format.\n",
        "Used for development purpose; had to restart notebokk several time, this method of reading is always faster.\n",
        "'''\n",
        "np.save('./Embeddings/embeddings.npy', embeddings)\n",
        "np.save('./Embeddings/labels.npy', labels)\n",
        "embeddings = np.load('./Embeddings/embeddings.npy').reshape(3750, 256)  #since 37547 items\n",
        "# labels = np.load('./Embeddings/labels.npy').reshape(3750,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxsL7ANJImsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Creating Dataframe with each embeddings elements as columns(total 256 columns) and coresponding labels as the index\n",
        "'''\n",
        "data = pd.DataFrame(embeddings)\n",
        "# data['labels'] = labels\n",
        "# data.set_index('labels', inplace=True)\n",
        "data.sort_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BfzfFQuImsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "7656a464-a15a-4e75-b1bd-657857c694cc"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.298233</td>\n",
              "      <td>3.890132</td>\n",
              "      <td>2.161420</td>\n",
              "      <td>2.919541</td>\n",
              "      <td>0.915389</td>\n",
              "      <td>0.762203</td>\n",
              "      <td>0.341757</td>\n",
              "      <td>1.779681</td>\n",
              "      <td>1.326085</td>\n",
              "      <td>1.977328</td>\n",
              "      <td>2.264809</td>\n",
              "      <td>-0.246840</td>\n",
              "      <td>1.698639</td>\n",
              "      <td>2.725577</td>\n",
              "      <td>1.572831</td>\n",
              "      <td>1.395808</td>\n",
              "      <td>1.113100</td>\n",
              "      <td>2.501344</td>\n",
              "      <td>1.017380</td>\n",
              "      <td>1.590476</td>\n",
              "      <td>0.663867</td>\n",
              "      <td>0.891124</td>\n",
              "      <td>0.122394</td>\n",
              "      <td>1.374564</td>\n",
              "      <td>1.413580</td>\n",
              "      <td>0.813698</td>\n",
              "      <td>1.497194</td>\n",
              "      <td>-0.150710</td>\n",
              "      <td>0.529689</td>\n",
              "      <td>1.298151</td>\n",
              "      <td>1.403920</td>\n",
              "      <td>1.316221</td>\n",
              "      <td>2.324769</td>\n",
              "      <td>3.569446</td>\n",
              "      <td>2.146585</td>\n",
              "      <td>3.035639</td>\n",
              "      <td>0.999953</td>\n",
              "      <td>0.633650</td>\n",
              "      <td>0.167108</td>\n",
              "      <td>1.519125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.918676</td>\n",
              "      <td>2.215043</td>\n",
              "      <td>2.349104</td>\n",
              "      <td>-0.265952</td>\n",
              "      <td>2.034925</td>\n",
              "      <td>2.917544</td>\n",
              "      <td>1.329232</td>\n",
              "      <td>1.069409</td>\n",
              "      <td>1.877034</td>\n",
              "      <td>2.796717</td>\n",
              "      <td>1.704482</td>\n",
              "      <td>2.219609</td>\n",
              "      <td>0.919194</td>\n",
              "      <td>0.462250</td>\n",
              "      <td>0.326423</td>\n",
              "      <td>1.126217</td>\n",
              "      <td>0.584120</td>\n",
              "      <td>1.647416</td>\n",
              "      <td>1.727820</td>\n",
              "      <td>-0.195034</td>\n",
              "      <td>1.436973</td>\n",
              "      <td>2.071370</td>\n",
              "      <td>0.958783</td>\n",
              "      <td>0.709829</td>\n",
              "      <td>1.915241</td>\n",
              "      <td>2.978550</td>\n",
              "      <td>1.724973</td>\n",
              "      <td>2.187523</td>\n",
              "      <td>0.951919</td>\n",
              "      <td>0.561665</td>\n",
              "      <td>0.443415</td>\n",
              "      <td>1.148996</td>\n",
              "      <td>0.789453</td>\n",
              "      <td>1.511401</td>\n",
              "      <td>1.853635</td>\n",
              "      <td>-0.204215</td>\n",
              "      <td>1.466831</td>\n",
              "      <td>2.055248</td>\n",
              "      <td>1.010053</td>\n",
              "      <td>0.819771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.528491</td>\n",
              "      <td>3.067372</td>\n",
              "      <td>1.349124</td>\n",
              "      <td>2.157776</td>\n",
              "      <td>0.733094</td>\n",
              "      <td>1.031408</td>\n",
              "      <td>0.116409</td>\n",
              "      <td>1.579189</td>\n",
              "      <td>1.552085</td>\n",
              "      <td>1.258545</td>\n",
              "      <td>1.845141</td>\n",
              "      <td>-0.195738</td>\n",
              "      <td>0.903636</td>\n",
              "      <td>1.866681</td>\n",
              "      <td>1.606041</td>\n",
              "      <td>1.454310</td>\n",
              "      <td>2.650052</td>\n",
              "      <td>3.970419</td>\n",
              "      <td>2.356035</td>\n",
              "      <td>3.055856</td>\n",
              "      <td>1.455854</td>\n",
              "      <td>0.713630</td>\n",
              "      <td>0.591817</td>\n",
              "      <td>1.405283</td>\n",
              "      <td>0.758587</td>\n",
              "      <td>2.299815</td>\n",
              "      <td>2.619401</td>\n",
              "      <td>-0.289073</td>\n",
              "      <td>2.163923</td>\n",
              "      <td>2.936829</td>\n",
              "      <td>1.260299</td>\n",
              "      <td>0.843638</td>\n",
              "      <td>2.226369</td>\n",
              "      <td>3.324731</td>\n",
              "      <td>1.891143</td>\n",
              "      <td>2.424775</td>\n",
              "      <td>1.287966</td>\n",
              "      <td>0.644547</td>\n",
              "      <td>0.544149</td>\n",
              "      <td>1.180470</td>\n",
              "      <td>...</td>\n",
              "      <td>0.619072</td>\n",
              "      <td>2.027874</td>\n",
              "      <td>2.522706</td>\n",
              "      <td>-0.270576</td>\n",
              "      <td>2.032414</td>\n",
              "      <td>2.729682</td>\n",
              "      <td>1.054362</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.989419</td>\n",
              "      <td>1.753560</td>\n",
              "      <td>0.736090</td>\n",
              "      <td>1.161368</td>\n",
              "      <td>0.653786</td>\n",
              "      <td>0.609178</td>\n",
              "      <td>0.014831</td>\n",
              "      <td>0.989532</td>\n",
              "      <td>0.800255</td>\n",
              "      <td>0.728179</td>\n",
              "      <td>1.125635</td>\n",
              "      <td>-0.118239</td>\n",
              "      <td>0.443240</td>\n",
              "      <td>0.952504</td>\n",
              "      <td>0.951747</td>\n",
              "      <td>0.836262</td>\n",
              "      <td>2.587093</td>\n",
              "      <td>3.714441</td>\n",
              "      <td>2.312010</td>\n",
              "      <td>2.851317</td>\n",
              "      <td>1.291653</td>\n",
              "      <td>0.607908</td>\n",
              "      <td>0.611469</td>\n",
              "      <td>1.285244</td>\n",
              "      <td>0.685806</td>\n",
              "      <td>2.154910</td>\n",
              "      <td>2.370450</td>\n",
              "      <td>-0.263960</td>\n",
              "      <td>2.087883</td>\n",
              "      <td>2.780727</td>\n",
              "      <td>1.119043</td>\n",
              "      <td>0.773237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.485835</td>\n",
              "      <td>3.037633</td>\n",
              "      <td>1.347512</td>\n",
              "      <td>1.968364</td>\n",
              "      <td>0.809675</td>\n",
              "      <td>0.834655</td>\n",
              "      <td>0.220104</td>\n",
              "      <td>1.430151</td>\n",
              "      <td>1.484402</td>\n",
              "      <td>1.039972</td>\n",
              "      <td>1.701874</td>\n",
              "      <td>-0.181100</td>\n",
              "      <td>0.756961</td>\n",
              "      <td>1.544744</td>\n",
              "      <td>1.435840</td>\n",
              "      <td>1.408249</td>\n",
              "      <td>1.521032</td>\n",
              "      <td>2.220090</td>\n",
              "      <td>1.287036</td>\n",
              "      <td>1.552375</td>\n",
              "      <td>0.894500</td>\n",
              "      <td>0.419887</td>\n",
              "      <td>0.417024</td>\n",
              "      <td>0.797013</td>\n",
              "      <td>0.438632</td>\n",
              "      <td>1.158281</td>\n",
              "      <td>1.486528</td>\n",
              "      <td>-0.158625</td>\n",
              "      <td>1.085051</td>\n",
              "      <td>1.543114</td>\n",
              "      <td>0.714767</td>\n",
              "      <td>0.466447</td>\n",
              "      <td>2.473787</td>\n",
              "      <td>3.741616</td>\n",
              "      <td>2.099991</td>\n",
              "      <td>2.727004</td>\n",
              "      <td>1.396579</td>\n",
              "      <td>0.744777</td>\n",
              "      <td>0.516173</td>\n",
              "      <td>1.351742</td>\n",
              "      <td>...</td>\n",
              "      <td>1.396214</td>\n",
              "      <td>0.823402</td>\n",
              "      <td>1.484940</td>\n",
              "      <td>-0.151738</td>\n",
              "      <td>0.533151</td>\n",
              "      <td>1.267503</td>\n",
              "      <td>1.339496</td>\n",
              "      <td>1.317615</td>\n",
              "      <td>1.742086</td>\n",
              "      <td>2.683860</td>\n",
              "      <td>1.567089</td>\n",
              "      <td>2.133707</td>\n",
              "      <td>0.839990</td>\n",
              "      <td>0.477664</td>\n",
              "      <td>0.236221</td>\n",
              "      <td>1.160970</td>\n",
              "      <td>0.609170</td>\n",
              "      <td>1.565798</td>\n",
              "      <td>1.620337</td>\n",
              "      <td>-0.186945</td>\n",
              "      <td>1.277311</td>\n",
              "      <td>1.935122</td>\n",
              "      <td>0.983941</td>\n",
              "      <td>0.767516</td>\n",
              "      <td>1.309186</td>\n",
              "      <td>2.968931</td>\n",
              "      <td>1.253853</td>\n",
              "      <td>1.757553</td>\n",
              "      <td>0.558413</td>\n",
              "      <td>0.875571</td>\n",
              "      <td>0.380745</td>\n",
              "      <td>1.458480</td>\n",
              "      <td>1.634853</td>\n",
              "      <td>0.886698</td>\n",
              "      <td>1.595138</td>\n",
              "      <td>-0.161276</td>\n",
              "      <td>0.575072</td>\n",
              "      <td>1.398164</td>\n",
              "      <td>1.503290</td>\n",
              "      <td>1.515547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.231559</td>\n",
              "      <td>2.405146</td>\n",
              "      <td>1.072657</td>\n",
              "      <td>1.376351</td>\n",
              "      <td>0.693235</td>\n",
              "      <td>0.647352</td>\n",
              "      <td>0.369635</td>\n",
              "      <td>0.972535</td>\n",
              "      <td>1.190208</td>\n",
              "      <td>0.635830</td>\n",
              "      <td>1.378442</td>\n",
              "      <td>-0.136158</td>\n",
              "      <td>0.550280</td>\n",
              "      <td>1.038194</td>\n",
              "      <td>1.042688</td>\n",
              "      <td>1.059472</td>\n",
              "      <td>1.894324</td>\n",
              "      <td>3.083202</td>\n",
              "      <td>1.733501</td>\n",
              "      <td>2.392312</td>\n",
              "      <td>0.895000</td>\n",
              "      <td>0.618567</td>\n",
              "      <td>0.268386</td>\n",
              "      <td>1.346599</td>\n",
              "      <td>0.908529</td>\n",
              "      <td>1.590256</td>\n",
              "      <td>1.818213</td>\n",
              "      <td>-0.203968</td>\n",
              "      <td>1.396023</td>\n",
              "      <td>2.117943</td>\n",
              "      <td>1.189890</td>\n",
              "      <td>1.057447</td>\n",
              "      <td>1.154417</td>\n",
              "      <td>2.387687</td>\n",
              "      <td>0.941946</td>\n",
              "      <td>1.504906</td>\n",
              "      <td>0.786411</td>\n",
              "      <td>0.773385</td>\n",
              "      <td>0.172120</td>\n",
              "      <td>1.212649</td>\n",
              "      <td>...</td>\n",
              "      <td>1.362502</td>\n",
              "      <td>0.777328</td>\n",
              "      <td>1.441523</td>\n",
              "      <td>-0.148762</td>\n",
              "      <td>0.495041</td>\n",
              "      <td>1.261205</td>\n",
              "      <td>1.379180</td>\n",
              "      <td>1.314130</td>\n",
              "      <td>1.764403</td>\n",
              "      <td>2.574659</td>\n",
              "      <td>1.550418</td>\n",
              "      <td>2.027153</td>\n",
              "      <td>0.948270</td>\n",
              "      <td>0.467570</td>\n",
              "      <td>0.298046</td>\n",
              "      <td>0.978281</td>\n",
              "      <td>0.488298</td>\n",
              "      <td>1.572729</td>\n",
              "      <td>1.673612</td>\n",
              "      <td>-0.185016</td>\n",
              "      <td>1.357624</td>\n",
              "      <td>1.931231</td>\n",
              "      <td>0.872495</td>\n",
              "      <td>0.610017</td>\n",
              "      <td>2.338455</td>\n",
              "      <td>3.488106</td>\n",
              "      <td>2.038767</td>\n",
              "      <td>2.657154</td>\n",
              "      <td>1.321091</td>\n",
              "      <td>0.656878</td>\n",
              "      <td>0.528615</td>\n",
              "      <td>1.253275</td>\n",
              "      <td>0.669298</td>\n",
              "      <td>1.929258</td>\n",
              "      <td>2.324919</td>\n",
              "      <td>-0.252677</td>\n",
              "      <td>1.797768</td>\n",
              "      <td>2.516529</td>\n",
              "      <td>1.099590</td>\n",
              "      <td>0.728694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.971261</td>\n",
              "      <td>1.819103</td>\n",
              "      <td>0.822844</td>\n",
              "      <td>1.118749</td>\n",
              "      <td>0.552627</td>\n",
              "      <td>0.554850</td>\n",
              "      <td>0.161039</td>\n",
              "      <td>0.920390</td>\n",
              "      <td>0.863664</td>\n",
              "      <td>0.610734</td>\n",
              "      <td>1.128219</td>\n",
              "      <td>-0.114067</td>\n",
              "      <td>0.469860</td>\n",
              "      <td>0.957540</td>\n",
              "      <td>0.895567</td>\n",
              "      <td>0.808498</td>\n",
              "      <td>1.865169</td>\n",
              "      <td>3.253450</td>\n",
              "      <td>1.618469</td>\n",
              "      <td>2.265320</td>\n",
              "      <td>1.060692</td>\n",
              "      <td>0.771006</td>\n",
              "      <td>0.349418</td>\n",
              "      <td>1.408452</td>\n",
              "      <td>1.080588</td>\n",
              "      <td>1.439622</td>\n",
              "      <td>2.095565</td>\n",
              "      <td>-0.220625</td>\n",
              "      <td>1.204769</td>\n",
              "      <td>1.914637</td>\n",
              "      <td>1.359648</td>\n",
              "      <td>1.024663</td>\n",
              "      <td>1.000934</td>\n",
              "      <td>2.007594</td>\n",
              "      <td>0.819135</td>\n",
              "      <td>1.352331</td>\n",
              "      <td>0.551482</td>\n",
              "      <td>0.731601</td>\n",
              "      <td>0.060129</td>\n",
              "      <td>1.229319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.701783</td>\n",
              "      <td>2.099050</td>\n",
              "      <td>2.126587</td>\n",
              "      <td>-0.243601</td>\n",
              "      <td>1.779061</td>\n",
              "      <td>2.545067</td>\n",
              "      <td>1.152991</td>\n",
              "      <td>0.817511</td>\n",
              "      <td>1.344196</td>\n",
              "      <td>1.998541</td>\n",
              "      <td>1.150417</td>\n",
              "      <td>1.504257</td>\n",
              "      <td>0.776477</td>\n",
              "      <td>0.401684</td>\n",
              "      <td>0.240775</td>\n",
              "      <td>0.796113</td>\n",
              "      <td>0.468990</td>\n",
              "      <td>1.087318</td>\n",
              "      <td>1.295557</td>\n",
              "      <td>-0.139155</td>\n",
              "      <td>0.952401</td>\n",
              "      <td>1.407791</td>\n",
              "      <td>0.687362</td>\n",
              "      <td>0.526374</td>\n",
              "      <td>0.862133</td>\n",
              "      <td>1.923445</td>\n",
              "      <td>0.757773</td>\n",
              "      <td>1.336653</td>\n",
              "      <td>0.423497</td>\n",
              "      <td>0.725814</td>\n",
              "      <td>0.065271</td>\n",
              "      <td>1.215843</td>\n",
              "      <td>1.160144</td>\n",
              "      <td>0.620955</td>\n",
              "      <td>1.179555</td>\n",
              "      <td>-0.122314</td>\n",
              "      <td>0.401666</td>\n",
              "      <td>1.043888</td>\n",
              "      <td>1.205431</td>\n",
              "      <td>1.131978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 256 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       253       254       255\n",
              "0  2.298233  3.890132  2.161420  ...  2.055248  1.010053  0.819771\n",
              "1  1.528491  3.067372  1.349124  ...  2.780727  1.119043  0.773237\n",
              "2  1.485835  3.037633  1.347512  ...  1.398164  1.503290  1.515547\n",
              "3  1.231559  2.405146  1.072657  ...  2.516529  1.099590  0.728694\n",
              "4  0.971261  1.819103  0.822844  ...  1.043888  1.205431  1.131978\n",
              "\n",
              "[5 rows x 256 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376i0q-KImsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEmbeddings(idx):\n",
        "  \n",
        "  '''\n",
        "  Calculates embedding vector given a single image\n",
        "  input: index of single image to be tested\n",
        "  return: Embeddings of shape (256,)\n",
        "  '''\n",
        "  IMG_SIZE = 64\n",
        "  \n",
        " \n",
        "  img = train_ar[idx].reshape(1,28,28,1)\n",
        "  img = img/255.0\n",
        "    \n",
        "#     img = cv2.imread('./train/im-400-{}.jpg'.format(idx))\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "#     img = img/255.0\n",
        "    \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "      saver = tf.train.Saver()\n",
        "      saver.restore(sess, tf.train.latest_checkpoint('./saved_model/'))\n",
        "      dist = sess.run(encoded,feed_dict={X:img.reshape((-1,28,28,1))})\n",
        "  \n",
        "  return np.squeeze(dist.reshape(-1,16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFnXqrB7Imsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading images with index of 'scores' dataframe\n",
        "def getPredictedImages(score):\n",
        "    '''\n",
        "    loading images with id equals to index of 'score'\n",
        "    '''\n",
        "    images = []\n",
        "    IMG_SIZE = 200\n",
        "    for idx in score.index:\n",
        "      \n",
        "      \n",
        "      img = train_ar[idx].reshape(1,28,28,1)\n",
        "      \n",
        "      \n",
        "#         path = './train/im-400-{}.jpg'.format(idx)\n",
        "#         img = Image.open(path)\n",
        "#         img  = make_square(img)\n",
        "#         img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#         img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "      images.append([np.array(img)])\n",
        "    return np.array(images).reshape(-1, 200,200,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA5kLOi_Imss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPredictions(testEmbeddings):\n",
        "    '''\n",
        "    input: Embeddings a single test image; shape- (256,)\n",
        "    return: Dataframe contains details of top 10 similar images\n",
        "    and \n",
        "    '''\n",
        "    dist = (np.array(data) - testEmbeddings)**2\n",
        "    dist = np.sum(dist, axis=1)\n",
        "    dist = np.sqrt(dist)    \n",
        "    '''\n",
        "    dist : Euclidean distance between testEmbedding and embeddings of all the images. dtype: numpy array\n",
        "    '''\n",
        "    \n",
        "    df = pd.DataFrame({'distance':dist})\n",
        "    df.index = data.index\n",
        "    df.sort_values('distance', ascending=True, inplace=True) \n",
        "    score = df[:10] #Taking highest 10 \n",
        "    score['score'] = df.distance[:10].apply(lambda x: np.round(1-np.tanh(x)**10, 3)).values\n",
        "    '''\n",
        "    tanh is used to map distance to 0 to 1\n",
        "    '''\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv6aYdmaImst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTestimage(idx):\n",
        "    '''\n",
        "    This is for ploting the original image\n",
        "    reads image from directory given index\n",
        "    '''\n",
        "    path = './train/im-400-{}.jpg'.format(idx)\n",
        "    img = Image.open(path)\n",
        "    img  = make_square(img)\n",
        "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hl7odeAImst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotImages(idx, im, images, score):\n",
        "    '''\n",
        "    Function to plot original image and images with highest scores; \n",
        "    plots 10 images with xlabels: Score, original id and seller-name \n",
        "    '''\n",
        "    fig, axes = plt.subplots(5,2, figsize = (15,25))\n",
        "    fig.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
        "    \n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i==0:\n",
        "            idX, sellerName = getinfo(idx)\n",
        "            ax.imshow(im, cmap = 'binary')\n",
        "            xlabel = \"Original image \\n id: {} \\n Seller: {}\".format(idX, sellerName)\n",
        "            ax.set_xlabel(xlabel, fontsize = 13)\n",
        "        else:    \n",
        "            ax.imshow(images[i-1], cmap = 'binary')\n",
        "            \n",
        "            idX, sellerName = getinfo(score.index[i])\n",
        "            xlabel = \"score: {} \\nid:{} \\n Seller: {}\".format(score.score.iloc[i-1], idX, sellerName)\n",
        "        \n",
        "            ax.set_xlabel(xlabel, fontsize = 12)\n",
        "        \n",
        "        #Remove axis ticks\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJchHKSSImsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# '''\n",
        "# Reading two Dataframes, 'Data' and 'tunicSlice'\n",
        "# Data : sliced 'tunic' from original with index had reset\n",
        "# tunicSlice : sliced 'tunic' from original with original index\n",
        "# '''\n",
        "# Data = pd.read_csv('./Data/data.csv')\n",
        "# tunicSlice = pd.read_csv('./Data/tunics.csv')\n",
        "\n",
        "# def getIndex(idx):\n",
        "#     '''\n",
        "#     index of subset and original data is different.\n",
        "#     This function will return original index given the subset index\n",
        "#     '''\n",
        "#     pid = Data[Data.index==idx]['productId'].values[0]\n",
        "#     idxTunic = tunicSlice[tunicSlice.productId==pid]['index'].values[0]\n",
        "#     return idxTunic\n",
        "\n",
        "# def getinfo(idx):\n",
        "#     pid = Data[Data.index==idx]['productId'].values[0]\n",
        "#     idxTunic = tunicSlice[tunicSlice.productId==pid]['index'].values[0]\n",
        "#     return idxTunic, tunicSlice[tunicSlice['index']==idxTunic]['sellerName'].values[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy7ltBxNImsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "09f29a25-2625-4533-b9dd-c8c748377e92"
      },
      "source": [
        "'''\n",
        "Testing one image\n",
        "'''\n",
        "idx = 4534\n",
        "testEmbeddings = getEmbeddings(idx)\n",
        "score = getPredictions(testEmbeddings)\n",
        "images =  getPredictedImages(score)\n",
        "im = getTestimage(idx)\n",
        "plotImages(idx, im, images, score)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-aac94aed65a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4534\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtestEmbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestEmbeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgetPredictedImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTestimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-222a97a722ac>\u001b[0m in \u001b[0;36mgetPredictions\u001b[0;34m(testEmbeddings)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtestEmbeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3750,256) (16,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOgw9y4aImsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 1732\n",
        "testEmbeddings = getEmbeddings(idx)\n",
        "im = getTestimage(idx)\n",
        "score = getPredictions(testEmbeddings)\n",
        "images =  getPredictedImages(score)\n",
        "plotImages(idx, im, images, score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqs5egByImsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#Calculating score for every item and writing it to dictionary as the given sample submission format.\n",
        "I havev made a cutt off score of 0.5; anything below 0.5 doesn't count.\n",
        "Only 500 of the duplicates are calculated and stored\n",
        "'''\n",
        "sc = []\n",
        "out = dict()\n",
        "for i in range(1):\n",
        "    testEmbeddings = getEmbeddings(i)\n",
        "    score = getPredictions(testEmbeddings)\n",
        "    ls = [[getIndex(score.index[i]), score.score.iloc[i]] for i in range(6) if score.score.iloc[i]>0.5 and score.index[i] != i]\n",
        "    ind, _ = getinfo(i)\n",
        "    out[str(ind)] = ls  \n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2SGfw-jImsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se_b2SpOImsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jdZKrInImsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}