{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from alpha import iterate_minibatches \n",
    "from cnn_utils import conv_layer,fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm as tqdm\n",
    "import os,cv2,random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 99\n",
    "IMG_SIZE = 64\n",
    "labels = []\n",
    "data = []\n",
    "DIR = \"./train\"\n",
    "for i, im in enumerate(os.listdir(DIR)):\n",
    "  if not im.startswith('.'):\n",
    "    path = os.path.join(DIR, im)\n",
    "    if i >= num_data : break\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "    img = img/255.0\n",
    "    data.append([np.array(img)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(data).reshape(-1,IMG_SIZE,IMG_SIZE,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./train.npy', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('./train.npy').reshape(99, 64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 17666194669463978261]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveGraph(graph):\n",
    "  with tf.Session(graph = graph) as sess:\n",
    "    filename = \"./summary_log/VS-\"+time.strftime(\"%H%M%S\")\n",
    "    writer = tf.summary.FileWriter(filename, sess.graph)\n",
    "  print(\"Tensorboard summary saved to \"+filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph1.as_default():\n",
    "    X = tf.placeholder(tf.float32,[None,64,64,3])\n",
    "    Y = tf.placeholder(tf.float32,[None,64,64,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(b,alpha=0.1):\n",
    "    return tf.maximum(alpha*b,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Encoder\n",
    "\n",
    "with graph1.as_default():\n",
    "  with tf.name_scope('en-convolutions'):\n",
    "    conv0 = tf.layers.conv2d(X, filters = 4, kernel_size  = (3, 3), strides = (1,1), padding = 'SAME', use_bias = True, activation = lrelu, name = 'conv0')\n",
    "  # 32*32*4\n",
    "  with tf.name_scope('en_pooling') :\n",
    "    maxpool0 = tf.layers.max_pooling2d(conv0, pool_size = (2, 2), strides = (2, 2),name = 'pool0')\n",
    "    \n",
    "  with tf.name_scope('en-convolutions'):\n",
    "    conv1 = tf.layers.conv2d(maxpool0, filters = 4, kernel_size  = (3, 3), strides = (1,1), padding = 'SAME', use_bias = True, activation = lrelu, name = 'conv1')\n",
    "  \n",
    "  # 32*32*4\n",
    "  with tf.name_scope('en_pooling') :\n",
    "    maxpool1 = tf.layers.max_pooling2d(conv1, pool_size = (2, 2), strides = (2, 2),name = 'pool0')\n",
    "  # 16*16*4 \n",
    "  with tf.name_scope('en-convolutions'):\n",
    "    conv2 = tf.layers.conv2d(maxpool1, filters = 8, kernel_size  = (3, 3), strides = (1,1), padding = 'SAME', use_bias = True, activation = lrelu, name = 'conv2')\n",
    "  # 16*16*8\n",
    "  with tf.name_scope('en_pooling') :\n",
    "    maxpool2 = tf.layers.max_pooling2d(conv2, pool_size = (2, 2), strides = (2, 2),name = 'pool0')\n",
    "  # 8*8*8\n",
    "  with tf.name_scope('en-convolutions'):\n",
    "    conv3 = tf.layers.conv2d(maxpool2, filters = 16, kernel_size  = (3, 3), strides = (1,1), padding = 'SAME', use_bias = True, activation = lrelu, name = 'conv3')\n",
    "  #  8x8x16\n",
    "  with tf.name_scope('encoding'):\n",
    "    encoded = tf.layers.average_pooling2d(conv3,pool_size=(2,2),strides=(2,2),name='encoding')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decoder\n",
    "\n",
    "with graph1.as_default():\n",
    "  with tf.name_scope('decoder'):\n",
    "    upsample1 = tf.layers.conv2d_transpose(encoded, filters = 16, kernel_size = 3, padding = 'SAME', strides = 2, name = 'upsample1')\n",
    "    # 8*8*16\n",
    "    conv4 = tf.layers.conv2d(upsample1, filters = 16, kernel_size = (3,3), strides = (1,1), padding = 'SAME', use_bias = True, name = 'conv4',activation =lrelu)\n",
    "    # 8x8x16\n",
    "    upsample2 = tf.layers.conv2d_transpose(conv4,filters=8,kernel_size=3,padding='same',strides=2,name='upsample2')\n",
    "    # 16x16x8\n",
    "    conv5 = tf.layers.conv2d(upsample2,filters=8,kernel_size=(3,3),strides=(1,1),name='conv5',padding='SAME',use_bias=True,activation=lrelu)\n",
    "    # 16x16x8\n",
    "    upsample3 = tf.layers.conv2d_transpose(conv5,filters=8,kernel_size=5,padding='same',strides=2,name='upsample3')\n",
    "    # 32x32x8\n",
    "    conv6 = tf.layers.conv2d(upsample3,filters=4,kernel_size=(5,5),strides=(1,1),name='conv6',padding='SAME',use_bias=True,activation=lrelu)\n",
    "        \n",
    "    # 16x16x8\n",
    "    upsample4 = tf.layers.conv2d_transpose(conv6,filters=8,kernel_size=5,padding='same',strides=2,name='upsample4')\n",
    "    # 32x32x8\n",
    "    conv7 = tf.layers.conv2d(upsample4,filters=4,kernel_size=(5,5),strides=(1,1),name='conv7',padding='SAME',use_bias=True,activation=lrelu)\n",
    "        \n",
    "        \n",
    "    # 32x32x4\n",
    "    logits = tf.layers.conv2d(conv7,filters=3,kernel_size=(3,3),strides=(1,1),name='logits',padding='SAME',use_bias=True)\n",
    "    #Now 32x32x3\n",
    "    # Pass logits through sigmoid to get reconstructed image\n",
    "    decoded = tf.sigmoid(logits,name='recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard summary saved to ./summary_log/VS-155013\n"
     ]
    }
   ],
   "source": [
    "saveGraph(graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph1.as_default():\n",
    "  loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "  lr = tf.placeholder(tf.float32, shape=[])\n",
    "  cost = tf.reduce_mean(loss) \n",
    "  opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training loss: 0.6936\n",
      "Epoch: 2/20... Training loss: 0.6936\n",
      "Epoch: 3/20... Training loss: 0.6934\n",
      "Epoch: 4/20... Training loss: 0.6929\n",
      "Epoch: 5/20... Training loss: 0.6925\n",
      "Epoch: 6/20... Training loss: 0.6921\n",
      "Epoch: 7/20... Training loss: 0.6917\n",
      "Epoch: 8/20... Training loss: 0.6913\n",
      "Epoch: 9/20... Training loss: 0.6910\n",
      "Epoch: 10/20... Training loss: 0.6906\n",
      "Epoch: 11/20... Training loss: 0.6902\n",
      "Epoch: 12/20... Training loss: 0.6897\n",
      "Epoch: 13/20... Training loss: 0.6892\n",
      "Epoch: 14/20... Training loss: 0.6886\n",
      "Epoch: 15/20... Training loss: 0.6880\n",
      "Epoch: 16/20... Training loss: 0.6872\n",
      "Epoch: 17/20... Training loss: 0.6863\n",
      "Epoch: 18/20... Training loss: 0.6853\n",
      "Epoch: 19/20... Training loss: 0.6842\n",
      "Epoch: 20/20... Training loss: 0.6830\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 256\n",
    "epoch = 20\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('./saved_model/'))\n",
    "    for ep in range(epoch):\n",
    "      avg_cost = 0\n",
    "      for i, minibatch in enumerate(iterate_minibatches(train, minibatch_size)):\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={X: minibatch,\n",
    "                                                             Y: minibatch,\n",
    "                                                            lr: learning_rate})\n",
    "        \n",
    "      print(\"Epoch: {}/{}...\".format(ep+1, epoch), \\\n",
    "                  \"Training loss: {:.4f}\".format(batch_cost))\n",
    "    saver.save(sess, \"./saved_model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
